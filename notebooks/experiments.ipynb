{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on Community Detection in RecSys\n",
    "\n",
    "This Jupyter notebook aims to conduct a series of experiments to evaluate how the performance of specific recommendation algorithms varies with the addition of community detectors. The experiments will be performed using the MovieLens 100k and Jester datasets. The scikit-surprise library will also be used.\n",
    "\n",
    "The main goal of these experiments is to verify whether the integration of community detection techniques in recommender systems can improve the recommendation accuracy. The algorithms will be evaluated based on RMSE, MSE and MAE metrics and the results will be saved in CSV format for further analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Importing needed libs\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Importing needed libs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from surprise import (\n",
    "    accuracy,\n",
    "    Reader,\n",
    "    Dataset,\n",
    "    CoClustering,\n",
    "    KNNBasic,\n",
    "    NMF,\n",
    "    SVD\n",
    ")\n",
    "from surprise.model_selection.split import ShuffleSplit\n",
    "from surprise.trainset import Trainset\n",
    "import networkx as nx\n",
    "from cdlib import algorithms \n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Setting up functions\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Setting up functions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncouple(train_set: Trainset, test_set: list):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        It takes in a Trainset-Surprise object and \n",
    "        a list of test set data, and returns two pandas \n",
    "        dataframes: one containing the training set \n",
    "        data, and the other containing the test set \n",
    "        data.\n",
    "    Input: \n",
    "        train_set: a Trainset object containing the \n",
    "        training set data. \n",
    "        test_set: a list containing the test set data\n",
    "    Output:\n",
    "        df_train: a pandas dataframe containing the \n",
    "        training set data, with columns 'uid', 'iid' \n",
    "        and 'rating'\n",
    "        df_test: a pandas dataframe containing the test \n",
    "        set data, with columns 'uid', 'iid', and 'rating'\n",
    "    \"\"\"\n",
    "    iterator = train_set.all_ratings()\n",
    "    df_train = pd.DataFrame(train_set.all_ratings(), columns=['uid', 'iid', 'rating'])\n",
    "    df_test = pd.DataFrame.from_records(test_set, columns = ['uid', 'iid', 'rating'])\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "\n",
    "def get_similarity_matrix(data: pd.DataFrame, index: List[str], columns: List[str], values: str, metric: str):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        It takes the ratings data and returns an\n",
    "        user-user similarity matrix.\n",
    "        Null data is filled with zero.\n",
    "    Input: \n",
    "        data: pandas DataFrame containing the data \n",
    "        to be transformed into a rating matrix\n",
    "        index: a list of strings representing the \n",
    "        column names that will be used as the index \n",
    "        columns: a list of strings representing the \n",
    "        column names that will be used as the columns \n",
    "        of the rating matrix.\n",
    "        values: a string representing the column name \n",
    "        metric: the metric to use when calculating \n",
    "        distance between instances\n",
    "    Output:\n",
    "        similarity_matrix: an pandas user-user similarity\n",
    "        matrix\n",
    "    \"\"\"\n",
    "    if metric not in ['cosine', 'euclidian', 'l1', 'l2']:\n",
    "        raise ValueError('Invalid metric. Please choose one of the following: cosine, euclidian, l1 or l2')\n",
    "    rating_matrix = data.pivot_table(index=index, columns=columns, values=values)\n",
    "    rating_matrix = rating_matrix.copy().fillna(0)\n",
    "    similarity_matrix = pairwise_distances(rating_matrix, rating_matrix, metric=metric)\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Setting up experiment algorithms \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Setting up experiment algorithms \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos_recommendation = {\n",
    "    'SVD': SVD(),\n",
    "    'k-NN': KNNBasic(), \n",
    "    'NMF': NMF(), \n",
    "    'Co-Clustering': CoClustering()\n",
    "}\n",
    "\n",
    "communities_detectors = {\n",
    "    'Not-Applicable': None,\n",
    "    'Louvain': algorithms.louvain,\n",
    "    'Paris': algorithms.paris,\n",
    "    'Surprise': algorithms.surprise_communities \n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a36e8aa94a4103b48e2f31b4d2c754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "General Progress:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb53dea879c4342af5fda481722bfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Size Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4062401ed0041629603d4304175e7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splits Progress: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e03c50ac310445992425f3c2dbc34da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Similarity Metric Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee1aa5b5e97422e9687644b9fc2c9e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Community Detector Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb092f3d4794aec8f0201ca57ebd450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Algorithm Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce78f14374b461792ef257f9117533a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Algorithm Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118550e5819947a9b75c1ac69eb9a086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Algorithm Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m algo_name, algo \u001b[39min\u001b[39;00m  tqdm(algos_recommendation\u001b[39m.\u001b[39mitems(), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAlgorithm Progress\u001b[39m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):            \n\u001b[0;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m community_detector \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m         trainpd, testpd \u001b[39m=\u001b[39m uncouple(trainset, testset)\n\u001b[0;32m     14\u001b[0m         similarity_matrix \u001b[39m=\u001b[39m get_similarity_matrix(trainpd, index\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39muid\u001b[39m\u001b[39m'\u001b[39m], columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39miid\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m     15\u001b[0m                                                 values\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m'\u001b[39m, metric\u001b[39m=\u001b[39msimilarity_metric)\n\u001b[0;32m     16\u001b[0m         G \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mfrom_numpy_matrix(similarity_matrix)\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36muncouple\u001b[1;34m(train_set, test_set)\u001b[0m\n\u001b[0;32m     22\u001b[0m i\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m (uid, iid, rating) \u001b[39min\u001b[39;00m iterator:\n\u001b[1;32m---> 24\u001b[0m     df_train\u001b[39m.\u001b[39;49mloc[i] \u001b[39m=\u001b[39m [uid, iid, rating]\n\u001b[0;32m     25\u001b[0m     i\u001b[39m=\u001b[39mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     26\u001b[0m df_test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_records(test_set, columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39muid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39miid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\core\\indexing.py:716\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    715\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 716\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\core\\indexing.py:1682\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     indexer, missing \u001b[39m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[0;32m   1681\u001b[0m     \u001b[39mif\u001b[39;00m missing:\n\u001b[1;32m-> 1682\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_with_indexer_missing(indexer, value)\n\u001b[0;32m   1683\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1685\u001b[0m \u001b[39m# align and set the values\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\core\\indexing.py:2020\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   2018\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_mgr \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39m_mgr\n\u001b[0;32m   2019\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2020\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_append(value)\u001b[39m.\u001b[39m_mgr\n\u001b[0;32m   2021\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_maybe_update_cacher(clear\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\core\\frame.py:9088\u001b[0m, in \u001b[0;36mDataFrame._append\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m   9085\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   9086\u001b[0m     to_concat \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m, other]\n\u001b[1;32m-> 9088\u001b[0m result \u001b[39m=\u001b[39m concat(\n\u001b[0;32m   9089\u001b[0m     to_concat,\n\u001b[0;32m   9090\u001b[0m     ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m   9091\u001b[0m     verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m   9092\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   9093\u001b[0m )\n\u001b[0;32m   9094\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   9095\u001b[0m     combined_columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   9096\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sort\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9101\u001b[0m     \u001b[39m# combined_columns.equals check is necessary for preserving dtype\u001b[39;00m\n\u001b[0;32m   9102\u001b[0m     \u001b[39m#  in test_crosstab_normalize\u001b[39;00m\n\u001b[0;32m   9103\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mreindex(combined_columns, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[0;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m    156\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[39m    along the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[39m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    348\u001b[0m         objs,\n\u001b[0;32m    349\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    350\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m    351\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m    352\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[0;32m    353\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[0;32m    354\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    355\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m    356\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    357\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    358\u001b[0m     )\n\u001b[0;32m    360\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:542\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverify_integrity \u001b[39m=\u001b[39m verify_integrity\n\u001b[0;32m    540\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n\u001b[1;32m--> 542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_axes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_new_axes()\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:612\u001b[0m, in \u001b[0;36m_Concatenator._get_new_axes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_new_axes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Index]:\n\u001b[0;32m    611\u001b[0m     ndim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_result_dim()\n\u001b[1;32m--> 612\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    613\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concat_axis \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_comb_axis(i)\n\u001b[0;32m    614\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ndim)\n\u001b[0;32m    615\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:613\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_new_axes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Index]:\n\u001b[0;32m    611\u001b[0m     ndim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_result_dim()\n\u001b[0;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m--> 613\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concat_axis \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_comb_axis(i)\n\u001b[0;32m    614\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ndim)\n\u001b[0;32m    615\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\_libs\\properties.pyx:37\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:668\u001b[0m, in \u001b[0;36m_Concatenator._get_concat_axis\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[39mreturn\u001b[39;00m idx\n\u001b[0;32m    667\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 668\u001b[0m     concat_axis \u001b[39m=\u001b[39m _concat_indexes(indexes)\n\u001b[0;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    670\u001b[0m     concat_axis \u001b[39m=\u001b[39m _make_concat_multiindex(\n\u001b[0;32m    671\u001b[0m         indexes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames\n\u001b[0;32m    672\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:686\u001b[0m, in \u001b[0;36m_concat_indexes\u001b[1;34m(indexes)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_concat_indexes\u001b[39m(indexes) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Index:\n\u001b[1;32m--> 686\u001b[0m     \u001b[39mreturn\u001b[39;00m indexes[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mappend(indexes[\u001b[39m1\u001b[39;49m:])\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5120\u001b[0m, in \u001b[0;36mIndex.append\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   5117\u001b[0m names \u001b[39m=\u001b[39m {obj\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m to_concat}\n\u001b[0;32m   5118\u001b[0m name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(names) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\n\u001b[1;32m-> 5120\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_concat(to_concat, name)\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5128\u001b[0m, in \u001b[0;36mIndex._concat\u001b[1;34m(self, to_concat, name)\u001b[0m\n\u001b[0;32m   5123\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5124\u001b[0m \u001b[39mConcatenate multiple Index objects.\u001b[39;00m\n\u001b[0;32m   5125\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5126\u001b[0m to_concat_vals \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39m_values \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m to_concat]\n\u001b[1;32m-> 5128\u001b[0m result \u001b[39m=\u001b[39m concat_compat(to_concat_vals)\n\u001b[0;32m   5130\u001b[0m is_numeric \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   5131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_backward_compat_public_numeric_index \u001b[39mand\u001b[39;00m is_numeric:\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py:151\u001b[0m, in \u001b[0;36mconcat_compat\u001b[1;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[0;32m    148\u001b[0m             to_concat \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m to_concat]\n\u001b[0;32m    149\u001b[0m             kinds \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mo\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m--> 151\u001b[0m result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(to_concat, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kinds \u001b[39mand\u001b[39;00m result\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    153\u001b[0m     \u001b[39m# GH#39817\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBehavior when concatenating bool-dtype and numeric-dtype arrays is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdeprecated; in a future version these will cast to object dtype \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    161\u001b[0m     )\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "problematic_execs=[]\n",
    "for dataset in tqdm(['ml-100k', 'jester'], desc='General Progress', leave=True):\n",
    "    data = Dataset.load_builtin(dataset)\n",
    "    for test_size in tqdm([0.25, 0.1, 0.01], desc='Test Size Progress', leave=False):\n",
    "        shuffle_split = ShuffleSplit(n_splits=100, test_size=test_size)\n",
    "        split_id = 1\n",
    "        for trainset, testset in tqdm(shuffle_split.split(data), desc='Splits Progress', leave=False): \n",
    "            for similarity_metric in  tqdm(['cosine', 'euclidian', 'l1', 'l2'], desc='Similarity Metric Progress', leave=False): \n",
    "                for detector_name, community_detector in tqdm(communities_detectors.items(), desc='Community Detector Progress', leave=False):\n",
    "                    for algo_name, algo in  tqdm(algos_recommendation.items(), desc='Algorithm Progress', leave=False):            \n",
    "                        if community_detector != None:\n",
    "                            trainpd, testpd = uncouple(trainset, testset)\n",
    "                            similarity_matrix = get_similarity_matrix(trainpd, index=['uid'], columns=['iid'], \n",
    "                                                                    values='rating', metric=similarity_metric)\n",
    "                            G = nx.from_numpy_matrix(similarity_matrix)\n",
    "                            for u, v in G.edges():\n",
    "                                similarity = similarity_matrix[u][v]\n",
    "                                G[u][v]['weight'] = similarity\n",
    "                            if communities_detectors == 'Paris':\n",
    "                                try:\n",
    "                                    coms = community_detector(G)\n",
    "                                except Exception as e:\n",
    "                                    problematic_execs.append([\n",
    "                                        'Problem with communiy detection', dataset, similarity_metric, \n",
    "                                        detector_name, algo_name, test_size, split_id\n",
    "                                    ])\n",
    "                                continue\n",
    "                            else:\n",
    "                                try:\n",
    "                                    coms = community_detector(G, weights='weight')\n",
    "                                except Exception as e:\n",
    "                                    problematic_execs.append([\n",
    "                                        'communiy detection', dataset, similarity_metric, \n",
    "                                        detector_name, algo_name, test_size, split_id\n",
    "                                    ])\n",
    "                                continue\n",
    "                            all_predictions = []\n",
    "                            for community in tqdm(coms.communities, desc='Communities progress', leave=False):\n",
    "                                train_community = trainpd[trainpd['uid'].isin(community)]\n",
    "                                test_community = testpd[testpd['uid'].isin([str(x) for x in community])]\n",
    "                                \n",
    "                                reader = Reader(rating_scale=(1, 5))\n",
    "                                train_surprise = Dataset.load_from_df(train_community[['uid', 'iid', 'rating']], \n",
    "                                                                        reader)\n",
    "                                train_surprise = train_surprise.build_full_trainset()\n",
    "                                test_surprise = list(test_community.itertuples(index=False, name=None))\n",
    "                                \n",
    "                                try:\n",
    "                                    algo.fit(train_surprise)\n",
    "                                except Exception as e:\n",
    "                                    problematic_execs.append([\n",
    "                                        'model fitting', dataset, similarity_metric, \n",
    "                                        detector_name, algo_name, test_size, split_id\n",
    "                                    ])\n",
    "                                continue\n",
    "                                try:    \n",
    "                                    predictions = algo.test(test_surprise)\n",
    "                                except Exception as e:\n",
    "                                    problematic_execs.append([\n",
    "                                        'model prediction', dataset, similarity_metric, \n",
    "                                        detector_name, algo_name, test_size, split_id\n",
    "                                    ])\n",
    "                                continue      \n",
    "                                all_predictions.extend(predictions)\n",
    "                            rmse_value = accuracy.rmse(all_predictions, verbose=False)\n",
    "                            mse_value = accuracy.mse(all_predictions, verbose=False)\n",
    "                            mae_value = accuracy.mae(all_predictions, verbose=False)\n",
    "                        else:\n",
    "                            try:\n",
    "                                algo.fit(trainset)\n",
    "                            except Exception as e:\n",
    "                                    problematic_execs.append([\n",
    "                                        'model fitting', dataset, similarity_metric, \n",
    "                                        detector_name, algo_name, test_size, split_id\n",
    "                                    ])\n",
    "                            continue\n",
    "                            try:\n",
    "                                predictions = algo.test(testset)\n",
    "                            except Exception as e:\n",
    "                                    problematic_execs.append([\n",
    "                                        'model prediction', dataset, similarity_metric, \n",
    "                                        detector_name, algo_name, test_size, split_id\n",
    "                                    ])\n",
    "                            continue\n",
    "                        rmse_value = accuracy.rmse(predictions, verbose=False)\n",
    "                        mse_value = accuracy.mse(predictions, verbose=False)\n",
    "                        mae_value = accuracy.mae(predictions, verbose=False)\n",
    "                        result_dict = {\n",
    "                            'dataset': dataset,\n",
    "                            'similarity_metric': similarity_metric,\n",
    "                            'community_detector': detector_name,\n",
    "                            'algorithm_rec': algo_name,\n",
    "                            'test_size': test_size,\n",
    "                            'split_id': split_id,\n",
    "                            'rmse': rmse_value,\n",
    "                            'mse': mse_value,\n",
    "                            'mae': mae_value\n",
    "                        }\n",
    "                        results.append(result_dict)\n",
    "                        split_id += 1\n",
    "\n",
    "\n",
    "# Saving results as .csv file\n",
    "df_results = pd.DataFrame(results)\n",
    "file_name = 'results.csv'\n",
    "notebook_dir = os.getcwd()\n",
    "outputs_dir = notebook_dir.replace('notebooks', 'outputs')\n",
    "file_path = os.path.join(outputs_dir, file_name)\n",
    "df_results.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_execs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9af17dd3974b7d3f07030b1611a67048f1f5084e02d6dc9f5527477bb4d7d2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
