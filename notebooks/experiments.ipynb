{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on Community Detection in RecSys\n",
    "\n",
    "This Jupyter notebook aims to conduct a series of experiments to evaluate how the performance of specific recommendation algorithms varies with the addition of community detectors. The experiments will be performed using the MovieLens 100k and Jester datasets. The scikit-surprise library will also be used.\n",
    "\n",
    "The main goal of these experiments is to verify whether the integration of community detection techniques in recommender systems can improve the recommendation accuracy. The algorithms will be evaluated based on RMSE, MSE and MAE metrics and the results will be saved in CSV format for further analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Importing needed libs\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Importing needed libs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from surprise import (\n",
    "    accuracy,\n",
    "    Reader,\n",
    "    Dataset,\n",
    "    CoClustering,\n",
    "    KNNBasic,\n",
    "    NMF,\n",
    "    SVD\n",
    ")\n",
    "from surprise.model_selection.split import ShuffleSplit\n",
    "from surprise.trainset import Trainset\n",
    "import networkx as nx\n",
    "from cdlib import algorithms \n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Setting up functions\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Setting up functions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncouple(train_set: Trainset, test_set: list):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        It takes in a Trainset-Surprise object and \n",
    "        a list of test set data, and returns two pandas \n",
    "        dataframes: one containing the training set \n",
    "        data, and the other containing the test set \n",
    "        data.\n",
    "    Input: \n",
    "        train_set: a Trainset object containing the \n",
    "        training set data. \n",
    "        test_set: a list containing the test set data\n",
    "    Output:\n",
    "        df_train: a pandas dataframe containing the \n",
    "        training set data, with columns 'uid', 'iid' \n",
    "        and 'rating'\n",
    "        df_test: a pandas dataframe containing the test \n",
    "        set data, with columns 'uid', 'iid', and 'rating'\n",
    "    \"\"\"\n",
    "    iterator = train_set.all_ratings()\n",
    "    df_train = pd.DataFrame(columns=['uid','iid','rating'])\n",
    "    i=0\n",
    "    for (uid, iid, rating) in iterator:\n",
    "        df_train.loc[i] = [uid, iid, rating]\n",
    "        i=i+1\n",
    "    df_test = pd.DataFrame.from_records(test_set, columns = ['uid', 'iid', 'rating'])\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "\n",
    "def get_similarity_matrix(data: pd.DataFrame, index: List[str], columns: List[str], values: str, metric: str):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        It takes the ratings data and returns an\n",
    "        user-user similarity matrix.\n",
    "        Null data is filled with zero.\n",
    "    Input: \n",
    "        data: pandas DataFrame containing the data \n",
    "        to be transformed into a rating matrix\n",
    "        index: a list of strings representing the \n",
    "        column names that will be used as the index \n",
    "        columns: a list of strings representing the \n",
    "        column names that will be used as the columns \n",
    "        of the rating matrix.\n",
    "        values: a string representing the column name \n",
    "        metric: the metric to use when calculating \n",
    "        distance between instances\n",
    "    Output:\n",
    "        similarity_matrix: an pandas user-user similarity\n",
    "        matrix\n",
    "    \"\"\"\n",
    "    if metric not in ['cosine', 'euclidian', 'l1', 'l2']:\n",
    "        raise ValueError('Invalid metric. Please choose one of the following: cosine, euclidian, l1 or l2')\n",
    "    rating_matrix = data.pivot_table(index=index, columns=columns, values=values)\n",
    "    rating_matrix = rating_matrix.copy().fillna(0)\n",
    "    similarity_matrix = pairwise_distances(rating_matrix, rating_matrix, metric=metric)\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Setting up experiment algorithms \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Setting up experiment algorithms \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos_recommendation = {\n",
    "    'SVD': SVD(),\n",
    "    'k-NN': KNNBasic(), \n",
    "    'NMF': NMF(), \n",
    "    'Co-Clustering': CoClustering()\n",
    "}\n",
    "\n",
    "communities_detectors = {\n",
    "    'Not-Applicable': None,\n",
    "    'Louvain': algorithms.louvain,\n",
    "    'Paris': algorithms.paris,\n",
    "    'Surprise': algorithms.surprise_communities \n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e93c0fbf26549f5bc065d6d060dbbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "General Progress:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac5fdfebf25414681663d181e292da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Similarity Metric Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e897d702e13e46318fa0ba8f614087af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Community Detector Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc0bc03da154452ab1ac34003c549bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Algorithm Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081f817240384b3d9673fda14fcfe786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Size Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4274408f2cfa4bb6af47c83fa0dd480a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splits Progress: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c03a3afe1d4baa9bd99497c5d92abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splits Progress: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 71\u001b[0m         algo\u001b[39m.\u001b[39;49mfit(trainset)\n\u001b[0;32m     72\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     73\u001b[0m             problematic_execs\u001b[39m.\u001b[39mappend([\n\u001b[0;32m     74\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mmodel fitting\u001b[39m\u001b[39m'\u001b[39m, dataset, similarity_metric, \n\u001b[0;32m     75\u001b[0m                 detector_name, algo_name, test_size, split_id\n\u001b[0;32m     76\u001b[0m             ])\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\surprise\\prediction_algorithms\\matrix_factorization.pyx:155\u001b[0m, in \u001b[0;36msurprise.prediction_algorithms.matrix_factorization.SVD.fit\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\surprise\\prediction_algorithms\\matrix_factorization.pyx:228\u001b[0m, in \u001b[0;36msurprise.prediction_algorithms.matrix_factorization.SVD.sgd\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rdutr\\anaconda3\\envs\\mscenv\\lib\\site-packages\\surprise\\trainset.py:194\u001b[0m, in \u001b[0;36mTrainset.all_ratings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mfor\u001b[39;00m u, u_ratings \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mur\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    193\u001b[0m     \u001b[39mfor\u001b[39;00m i, r \u001b[39min\u001b[39;00m u_ratings:\n\u001b[1;32m--> 194\u001b[0m         \u001b[39myield\u001b[39;00m u, i, r\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "problematic_execs=[]\n",
    "for dataset in tqdm(['ml-100k', 'jester'], desc='General Progress', leave=True):\n",
    "    data = Dataset.load_builtin(dataset)\n",
    "    for test_size in tqdm([0.25, 0.1, 0.01], desc='Test Size Progress', leave=False):\n",
    "        shuffle_split = ShuffleSplit(n_splits=100, test_size=test_size)\n",
    "        split_id = 1\n",
    "        for trainset, testset in tqdm(shuffle_split.split(data), desc='Splits Progress', leave=False): \n",
    "            for similarity_metric in  tqdm(['cosine', 'euclidian', 'l1', 'l2'], desc='Similarity Metric Progress', leave=False): \n",
    "                for detector_name, community_detector in tqdm(communities_detectors.items(), desc='Community Detector Progress', leave=False):\n",
    "                    for algo_name, algo in  tqdm(algos_recommendation.items(), desc='Algorithm Progress', leave=False):            \n",
    "                        if community_detector != None:\n",
    "                            trainpd, testpd = uncouple(trainset, testset)\n",
    "                            similarity_matrix = get_similarity_matrix(trainpd, index=['uid'], columns=['iid'], \n",
    "                                                                    values='rating', metric=similarity_metric)\n",
    "                            G = nx.from_numpy_matrix(similarity_matrix)\n",
    "                            for u, v in G.edges():\n",
    "                                similarity = similarity_matrix[u][v]\n",
    "                                G[u][v]['weight'] = similarity\n",
    "                            if communities_detectors == 'Paris':\n",
    "                                try:\n",
    "                                    coms = community_detector(G)\n",
    "                                except Exception as e:\n",
    "                                    problematic_execs.append([\n",
    "                                        'Problem with communiy detection', dataset, similarity_metric, \n",
    "                                        detector_name, algo_name, test_size, split_id\n",
    "                                    ])\n",
    "                                continue\n",
    "                            else:\n",
    "                                try:\n",
    "                                    coms = community_detector(G, weights='weight')\n",
    "                                except Exception as e:\n",
    "                                    problematic_execs.append([\n",
    "                                        'communiy detection', dataset, similarity_metric, \n",
    "                                        detector_name, algo_name, test_size, split_id\n",
    "                                    ])\n",
    "                                continue\n",
    "                            all_predictions = []\n",
    "                            for community in tqdm(coms.communities, desc='Communities progress', leave=False):\n",
    "                                train_community = trainpd[trainpd['uid'].isin(community)]\n",
    "                                test_community = testpd[testpd['uid'].isin([str(x) for x in community])]\n",
    "                                \n",
    "                                reader = Reader(rating_scale=(1, 5))\n",
    "                                train_surprise = Dataset.load_from_df(train_community[['uid', 'iid', 'rating']], \n",
    "                                                                        reader)\n",
    "                                train_surprise = train_surprise.build_full_trainset()\n",
    "                                test_surprise = list(test_community.itertuples(index=False, name=None))\n",
    "                                \n",
    "                                try:\n",
    "                                    algo.fit(train_surprise)\n",
    "                                except Exception as e:\n",
    "                                    problematic_execs.append([\n",
    "                                        'model fitting', dataset, similarity_metric, \n",
    "                                        detector_name, algo_name, test_size, split_id\n",
    "                                    ])\n",
    "                                continue\n",
    "                                try:    \n",
    "                                    predictions = algo.test(test_surprise)\n",
    "                                except Exception as e:\n",
    "                                    problematic_execs.append([\n",
    "                                        'model prediction', dataset, similarity_metric, \n",
    "                                        detector_name, algo_name, test_size, split_id\n",
    "                                    ])\n",
    "                                continue      \n",
    "                                all_predictions.extend(predictions)\n",
    "                            rmse_value = accuracy.rmse(all_predictions, verbose=False)\n",
    "                            mse_value = accuracy.mse(all_predictions, verbose=False)\n",
    "                            mae_value = accuracy.mae(all_predictions, verbose=False)\n",
    "                        else:\n",
    "                            try:\n",
    "                                algo.fit(trainset)\n",
    "                            except Exception as e:\n",
    "                                    problematic_execs.append([\n",
    "                                        'model fitting', dataset, similarity_metric, \n",
    "                                        detector_name, algo_name, test_size, split_id\n",
    "                                    ])\n",
    "                            continue\n",
    "                            try:\n",
    "                                predictions = algo.test(testset)\n",
    "                            except Exception as e:\n",
    "                                    problematic_execs.append([\n",
    "                                        'model prediction', dataset, similarity_metric, \n",
    "                                        detector_name, algo_name, test_size, split_id\n",
    "                                    ])\n",
    "                            continue\n",
    "                        rmse_value = accuracy.rmse(predictions, verbose=False)\n",
    "                        mse_value = accuracy.mse(predictions, verbose=False)\n",
    "                        mae_value = accuracy.mae(predictions, verbose=False)\n",
    "                        result_dict = {\n",
    "                            'dataset': dataset,\n",
    "                            'similarity_metric': similarity_metric,\n",
    "                            'community_detector': detector_name,\n",
    "                            'algorithm_rec': algo_name,\n",
    "                            'test_size': test_size,\n",
    "                            'split_id': split_id,\n",
    "                            'rmse': rmse_value,\n",
    "                            'mse': mse_value,\n",
    "                            'mae': mae_value\n",
    "                        }\n",
    "                        results.append(result_dict)\n",
    "                        split_id += 1\n",
    "\n",
    "\n",
    "# Saving results as .csv file\n",
    "df_results = pd.DataFrame(results)\n",
    "file_name = 'results.csv'\n",
    "notebook_dir = os.getcwd()\n",
    "outputs_dir = notebook_dir.replace('notebooks', 'outputs')\n",
    "file_path = os.path.join(outputs_dir, file_name)\n",
    "df_results.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_execs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9af17dd3974b7d3f07030b1611a67048f1f5084e02d6dc9f5527477bb4d7d2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
