{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Enviroment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Dealing with dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import networkx as nx\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from cdlib import algorithms\n",
    "from functions import (split_in_k_folds, get_number_of_keys, get_user_item_matrices, get_adjacency_matrices)\n",
    "from surprise import (accuracy, Dataset, Reader, KNNBaseline, NormalPredictor, KNNBasic, KNNWithMeans, KNNWithZScore, \n",
    "                            KNNBaseline, SVD, SVDpp, NMF, SlopeOne, CoClustering, BaselineOnly)\n",
    "from surprise.model_selection import PredefinedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "surprise_algorithms = [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_ml100k():\n",
    "    dir = os.getcwd()+'/Datasets'\n",
    "    archive = '/u.data'\n",
    "    names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    m100k = pd.read_table(dir + archive, sep='\\t', \n",
    "                         header=None,\n",
    "                         names=names, engine='python')\n",
    "    num_users = m100k.user_id.unique().shape[0]\n",
    "    num_items = m100k.item_id.unique().shape[0]\n",
    "    return m100k, num_users, num_items\n",
    "m100k, num_users, num_items = read_data_ml100k()\n",
    "print(f'* * * A new file has been loaded. The dataframe contains {num_users} users and {num_items} items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m100k.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Spliting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = split_in_k_folds(m100k[['user_id','item_id' ]], m100k['rating'], k=folds, test_ratio=0.3)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pré-modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rating_matrices = get_user_item_matrices(splits)\n",
    "adjacency_matrices = get_adjacency_matrices(rating_matrices, metric_dist=\"cosine\", threshold=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G={}\n",
    "for i in range(len(adjacency_matrices.items())):   \n",
    "  G[f'graph-{i}'] = nx.from_numpy_matrix(adjacency_matrices[f'AM{0}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coms_louvain={}\n",
    "for i in range(len(G.items())):   \n",
    "  coms_louvain[i] = algorithms.louvain(G[f'graph-{i}'], weight='weight', resolution=1., randomize=False)\n",
    "coms_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(coms_louvain)):\n",
    "    subcommunity=[]\n",
    "    for u in splits[f'X_train{i}']['user_id']-1:\n",
    "        for c in coms_louvain[i].communities:\n",
    "            if u in c:\n",
    "                subcommunity.append(coms_louvain[i].communities.index(c))\n",
    "    splits[f'X_train{i}']['community']=subcommunity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Create surprise train/test objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainset = {}\n",
    "testset = {}\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "for i in range(folds): \n",
    "\n",
    "    print(f'*-- Executing process for fold 0{i}')# ele ta sobrescrevendo\n",
    "    train = splits[f'X_train{i}']\n",
    "    ytrain = splits[f'y_train{i}']\n",
    "    \n",
    "    test = splits[f'X_test{i}']\n",
    "    on_left = splits[f'X_train{i}'][['user_id', 'community']]\n",
    "    on_left = on_left.drop_duplicates()\n",
    "    test_merged = pd.merge(test, \n",
    "                    on_left, \n",
    "                    on ='user_id', \n",
    "                    how ='inner')\n",
    "    test=test_merged.set_index(test.index)   \n",
    "    ytest = splits[f'y_test{i}']\n",
    "\n",
    "    cmts = train[\"community\"].unique()\n",
    "    cmts = cmts.tolist()\n",
    "    print(f'---*-- {len(cmts)} subcommunities identified.')\n",
    "    for cmt in cmts:\n",
    "        print(f'------*-- Executing process for subcommunity 0{cmt}')\n",
    "        train_ = train[train['community'] == cmt]\n",
    "        ytrain_ = ytrain[ytrain.index.isin(train.index)]\n",
    "        train_['rating'] = ytrain_\n",
    "        train_ = train_.drop(['community'], axis=1) \n",
    "        data = Dataset.load_from_df(train_[['user_id', 'item_id', 'rating']], reader)\n",
    "        trainset[f'train_fd-{i}_sb-{cmt}'] = data.build_full_trainset()\n",
    "              \n",
    "        test_ = test[test['community'] == cmt]\n",
    "        ytest_ = ytest[ytest.index.isin(test.index)]\n",
    "        test_['rating'] = ytest_\n",
    "        test_ = test_.drop(['community'], axis=1) \n",
    "        test_[['user_id','item_id']] = test_[['user_id','item_id']].astype(str)\n",
    "        test_[['rating']] = test_[['rating']].astype(float)\n",
    "        test_ = test_.to_numpy()\n",
    "        result = list([tuple(e) for e in test_])\n",
    "        #data = Dataset.load_from_df(test[['user_id', 'item_id', 'rating']], reader)\n",
    "        testset[f'test_fd-{i}_sb-{cmt}'] = result #data.build_full_trainset()\n",
    "        print('**********')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tds os y_pred estão iguais .-.  pq?\n",
    "train = trainset['train_fd-0_sb-3']\n",
    "test = testset['test_fd-0_sb-3']\n",
    "algo = SVD()\n",
    "algo.fit(train)\n",
    "predictions = algo.test(test)\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
